"""
Web interface for protein localization pipeline using Gradio
Allows unrestricted TIFF file uploads and end-to-end processing
All outputs stored to /mnt/d/5TH_SEM/CELLULAR/output/output
"""
import gradio as gr
import numpy as np
import torch
from pathlib import Path
import tempfile
import os
import sys
from datetime import datetime

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from preprocessing.segmentation import TIFFLoader, CellposeSegmenter
from preprocessing.feature_extraction import FeatureExtractor, FeatureStorage
from graph_construction.graph_builder import GraphConstructor, PyTorchGeometricConverter, GraphStorage
from visualization.plotters import SegmentationVisualizer, StatisticalPlotter
from visualization.graph_viz import GraphVisualizer
from visualization.metrics import MetricsEvaluator
import config


class ProteinLocalizationInterface:
    """Interface for protein localization pipeline"""
    
    def __init__(self, model_path: str = None, output_dir: str = None):
        """
        Initialize interface
        
        Args:
            model_path: Path to trained model (optional)
            output_dir: Directory to store all outputs (default: from config)
        """
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        
        # Set output directory for interface files
        self.output_dir = output_dir or config.OUTPUT_DIR
        self.interface_dir = os.path.join(self.output_dir, "interface_outputs")
        self.viz_dir = os.path.join(self.output_dir, "visualizations")
        self.features_dir = os.path.join(self.output_dir, "features")
        self.graphs_dir = os.path.join(self.output_dir, "graphs")
        
        # Create output directories
        for directory in [self.interface_dir, self.viz_dir, self.features_dir, self.graphs_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # Initialize components
        self.loader = TIFFLoader()
        self.segmenter = CellposeSegmenter()
        self.feature_extractor = FeatureExtractor()
        self.graph_constructor = GraphConstructor()
        
        # Initialize visualization and storage components
        self.seg_viz = SegmentationVisualizer(output_dir=self.viz_dir)
        self.graph_viz = GraphVisualizer(output_dir=self.viz_dir)
        self.feature_storage = FeatureStorage(output_dir=self.features_dir)
        self.graph_storage = GraphStorage(output_dir=self.graphs_dir)
    
    def load_model(self, model_path: str):
        """Load trained model"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            # Model loading logic here
            print(f"Model loaded from {model_path}")
        except Exception as e:
            print(f"Error loading model: {e}")
    
    def process_tiff(self, tiff_file):
        """
        Process uploaded TIFF file through the entire pipeline
        
        Args:
            tiff_file: Uploaded TIFF file
        
        Returns:
            Tuple of (results_text, seg_image, graph_image, metrics_dict)
        """
        try:
            # Generate timestamp for unique filenames
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            input_filename = Path(tiff_file.name).stem
            base_name = f"{input_filename}_{timestamp}"
            
            results = []
            results.append(f"üìÇ Processing: {Path(tiff_file.name).name}")
            results.append(f"üìÅ Output directory: {self.output_dir}")
            results.append("")
            
            # Load TIFF
            image = self.loader.load_tiff(tiff_file.name)
            if image is None:
                return "Error: Could not load TIFF file", None, None, {}
            
            results.append(f"‚úì Loaded TIFF image: {image.shape}")
            
            # Segmentation
            masks, seg_info = self.segmenter.segment_image(image)
            if masks is None:
                return "Error: Segmentation failed", None, None, {}
            
            results.append(f"‚úì Segmentation complete: {seg_info['num_cells']} cells detected")
            
            # Feature extraction
            features = self.feature_extractor.extract_all_features(image, masks)
            if features.empty:
                return "Error: Feature extraction failed", None, None, {}
            
            results.append(f"‚úì Extracted {len(features.columns)} features from {len(features)} regions")
            
            # Save features
            self.feature_storage.save_features(features, f"{base_name}_features")
            results.append(f"‚úì Features saved to: {self.features_dir}")
            
            # Graph construction
            graph = self.graph_constructor.construct_graph(features, masks)
            results.append(f"‚úì Constructed graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges")
            
            # Save graph
            self.graph_storage.save_graph(graph, base_name)
            results.append(f"‚úì Graph saved to: {self.graphs_dir}")
            
            # Create visualizations
            seg_filename = f"{base_name}_segmentation.png"
            self.seg_viz.plot_segmentation_overlay(
                image, masks,
                title=f"Segmentation: {input_filename}",
                filename=seg_filename
            )
            seg_path = os.path.join(self.viz_dir, seg_filename)
            
            graph_filename = f"{base_name}_graph.png"
            self.graph_viz.plot_graph(
                graph,
                title=f"Graph: {input_filename}",
                filename=graph_filename
            )
            graph_path = os.path.join(self.viz_dir, graph_filename)
            
            results.append(f"‚úì Visualizations saved to: {self.viz_dir}")
            
            # Prediction (if model is loaded)
            prediction = "No model loaded - Upload a trained model to get predictions"
            confidence = 0.0
            predicted_class = "N/A"
            
            if self.model is not None:
                # Run inference with the actual model
                converter = PyTorchGeometricConverter()
                graph_data = converter.to_pytorch_geometric(graph)
                
                self.model.eval()
                with torch.no_grad():
                    x = graph_data['x'].to(self.device)
                    edge_index = graph_data['edge_index'].to(self.device)
                    output = self.model(x, edge_index)
                    predicted_class = output.argmax(dim=1).item()
                    confidence = torch.softmax(output, dim=1).max().item()
                
                prediction = f"Class {predicted_class}"
            
            results.append(f"\nüìä Prediction: {prediction}")
            results.append(f"üìä Confidence: {confidence:.2%}")
            
            # Prepare detailed metrics
            node_degrees = [graph.degree(n) for n in graph.nodes()]
            
            metrics = {
                'input_file': Path(tiff_file.name).name,
                'output_directory': self.output_dir,
                'image_shape': str(image.shape),
                'num_cells': seg_info['num_cells'],
                'num_nodes': graph.number_of_nodes(),
                'num_edges': graph.number_of_edges(),
                'avg_degree': 2 * graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else 0,
                'min_degree': min(node_degrees) if node_degrees else 0,
                'max_degree': max(node_degrees) if node_degrees else 0,
                'graph_density': graph.number_of_edges() / (graph.number_of_nodes() * (graph.number_of_nodes()-1)/2) if graph.number_of_nodes() > 1 else 0,
                'prediction': prediction,
                'predicted_class': predicted_class,
                'confidence': f"{confidence:.2%}",
                'num_features': len(features.columns),
                'num_regions': len(features),
                'saved_files': {
                    'features': f"{self.features_dir}/{base_name}_features.csv",
                    'graph': f"{self.graphs_dir}/{base_name}.gpickle",
                    'segmentation_viz': f"{self.viz_dir}/{seg_filename}",
                    'graph_viz': f"{self.viz_dir}/{graph_filename}"
                }
            }
            
            # Format results with file paths
            results_text = "\n".join(results)
            results_text += "\n\nüìà Graph Statistics:"
            results_text += f"\n  - Average degree: {metrics['avg_degree']:.2f}"
            results_text += f"\n  - Degree range: [{metrics['min_degree']}, {metrics['max_degree']}]"
            results_text += f"\n  - Graph density: {metrics['graph_density']:.4f}"
            results_text += "\n\nüíæ All files saved successfully!"
            
            return results_text, seg_path, graph_path, metrics
            
        except Exception as e:
            import traceback
            error_msg = f"Error processing file: {str(e)}\n{traceback.format_exc()}"
            return error_msg, None, None, {}
    
    def create_interface(self):
        """Create Gradio interface"""
        
        with gr.Blocks(title="Protein Sub-Cellular Localization", theme=gr.themes.Soft()) as interface:
            gr.Markdown("""
            # üß¨ Protein Sub-Cellular Localization Pipeline
            
            Upload any TIFF microscopy image (**no size restrictions**) for automatic segmentation, 
            feature extraction, graph construction, and localization prediction.
            
            ### Pipeline Steps:
            1. **Segmentation**: Detect neuronal structures (soma, dendrites, axons) using Cellpose
            2. **Feature Extraction**: Extract spatial, morphological, and intensity features
            3. **Graph Construction**: Build graph representation with nodes and edges
            4. **Prediction**: Classify protein localization (requires trained model)
            
            ### Output Storage:
            All processed files are automatically saved to: **`/mnt/d/5TH_SEM/CELLULAR/output/output`**
            - Features: CSV, HDF5, Pickle formats
            - Graphs: GraphML and Pickle formats
            - Visualizations: High-resolution PNG images (300 DPI)
            """)
            
            with gr.Row():
                with gr.Column():
                    # Input
                    tiff_input = gr.File(
                        label="üì§ Upload TIFF Image",
                        file_types=[".tif", ".tiff", ".TIF", ".TIFF"],
                        type="filepath"
                    )
                    
                    process_btn = gr.Button("üöÄ Process Image", variant="primary", size="lg")
                    
                    gr.Markdown("""
                    **Supported formats**: .tif, .tiff (any case)
                    
                    **‚ú® No file size restrictions** - Upload files of any size
                    
                    **Processing includes**:
                    - Cell segmentation and region detection
                    - Multi-channel feature extraction
                    - Graph-based spatial analysis
                    - Automated prediction (if model loaded)
                    """)
                
                with gr.Column():
                    # Results
                    results_text = gr.Textbox(
                        label="üìä Processing Results & Output Locations",
                        lines=18,
                        max_lines=25,
                        show_copy_button=True
                    )
            
            with gr.Row():
                # Visualizations
                seg_output = gr.Image(
                    label="üî¨ Segmentation Overlay", 
                    type="filepath",
                    show_download_button=True
                )
                graph_output = gr.Image(
                    label="üï∏Ô∏è Graph Visualization", 
                    type="filepath",
                    show_download_button=True
                )
            
            with gr.Row():
                # Detailed Metrics
                metrics_json = gr.JSON(
                    label="üìà Detailed Metrics & File Paths",
                    show_label=True
                )
            
            # Event handlers
            process_btn.click(
                fn=self.process_tiff,
                inputs=[tiff_input],
                outputs=[results_text, seg_output, graph_output, metrics_json]
            )
            
            gr.Markdown("""
            ---
            ### üéØ About This Pipeline
            
            This pipeline implements a complete workflow for analyzing 4D TIFF microscopy images:
            
            - **Preprocessing**: Recursive directory scanning and TIFF loading (2D/3D/4D support)
            - **Segmentation**: Cellpose-based detection of cellular structures and sub-compartments
            - **Feature Extraction**: Spatial, morphological, intensity, and region-level features
            - **Graph Construction**: Biologically meaningful graphs with spatial relationships
            - **Model Inference**: Graph-CNN, VGG-16, and hybrid CNN+GNN architectures
            - **Visualization**: Publication-ready plots and graphs (300 DPI)
            
            ### üíæ Output Directory Structure:
            ```
            /mnt/d/5TH_SEM/CELLULAR/output/output/
            ‚îú‚îÄ‚îÄ interface_outputs/     # Session results
            ‚îú‚îÄ‚îÄ visualizations/        # All plots and images
            ‚îú‚îÄ‚îÄ features/             # Extracted features (CSV, HDF5, Pickle)
            ‚îú‚îÄ‚îÄ graphs/               # Graph structures (GraphML, Pickle)
            ‚îî‚îÄ‚îÄ models/               # Trained models
            ```
            
            ### üìã Node Labels & Feature Summary:
            
            Each processed image generates:
            - **Node labels**: Stable identifiers for each detected region/compartment
            - **Feature vectors**: 20+ features per node (spatial, morphological, intensity)
            - **Edge relationships**: Spatial proximity and biological adjacency
            - **Evaluation metrics**: Accuracy, precision, recall, F1-score, specificity
            
            All outputs include node labels and can be used for:
            - Training new models
            - Analyzing protein distribution patterns
            - Publication-ready visualizations
            - Further computational analysis
            """)
        
        return interface


def launch_interface(model_path: str = None, output_dir: str = None, share: bool = False):
    """
    Launch the Gradio interface
    
    Args:
        model_path: Path to trained model (optional)
        output_dir: Directory to store all outputs (default: from config)
        share: Whether to create public link
    """
    app = ProteinLocalizationInterface(model_path=model_path, output_dir=output_dir)
    interface = app.create_interface()
    
    print(f"\n{'='*60}")
    print("üöÄ Launching Protein Localization Interface")
    print(f"{'='*60}")
    print(f"üìÅ Output directory: {app.output_dir}")
    print(f"üåê Server: http://localhost:7860")
    print(f"üî¨ Ready to process TIFF files (no size restrictions)")
    print(f"{'='*60}\n")
    
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=share,
        inbrowser=True
    )


if __name__ == "__main__":
    print("Starting Protein Localization Interface...")
    launch_interface(share=False)
