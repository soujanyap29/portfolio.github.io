{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Sub-Cellular Localization in Neurons\n",
    "## Complete Pipeline for 4D TIFF Microscopy Image Analysis\n",
    "\n",
    "This notebook demonstrates the complete end-to-end pipeline for analyzing protein sub-cellular localization in neuronal microscopy images.\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **Preprocessing**: Load and segment TIFF images\n",
    "2. **Feature Extraction**: Extract biological and computational features\n",
    "3. **Graph Construction**: Build graph representations\n",
    "4. **Model Training**: Train Graph-CNN and VGG-16 models\n",
    "5. **Evaluation**: Compute metrics and visualizations\n",
    "6. **Inference**: Predict localization on new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from preprocessing.segmentation import DirectoryHandler, TIFFLoader, CellposeSegmenter\n",
    "from preprocessing.feature_extraction import FeatureExtractor, FeatureStorage\n",
    "from graph_construction.graph_builder import GraphConstructor, PyTorchGeometricConverter, GraphStorage\n",
    "from models.graph_cnn import GraphCNN\n",
    "from models.vgg16 import VGG16Classifier\n",
    "from models.combined_model import CombinedModel\n",
    "from models.trainer import ModelTrainer, ProteinLocalizationDataset, create_data_loaders\n",
    "from visualization.plotters import SegmentationVisualizer, StatisticalPlotter, ColocalizationAnalyzer\n",
    "from visualization.graph_viz import GraphVisualizer\n",
    "from visualization.metrics import MetricsEvaluator\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "INPUT_DIR = \"/mnt/d/5TH_SEM/CELLULAR/input\"\n",
    "OUTPUT_DIR = \"/mnt/d/5TH_SEM/CELLULAR/output/output\"\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "VIZ_DIR = os.path.join(OUTPUT_DIR, \"visualizations\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(VIZ_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "NUM_CLASSES = 10  # Adjust based on your dataset\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"✓ Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan directory for TIFF files\n",
    "print(\"Scanning for TIFF files...\")\n",
    "dir_handler = DirectoryHandler(INPUT_DIR)\n",
    "tiff_files = dir_handler.scan_directory()\n",
    "\n",
    "print(f\"Found {len(tiff_files)} TIFF files\")\n",
    "if len(tiff_files) > 0:\n",
    "    print(f\"Sample files: {tiff_files[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and segment images\n",
    "loader = TIFFLoader()\n",
    "segmenter = CellposeSegmenter(model_type='cyto2')\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Process first image as example\n",
    "if len(tiff_files) > 0:\n",
    "    example_file = tiff_files[0]\n",
    "    print(f\"\\nProcessing example: {example_file}\")\n",
    "    \n",
    "    # Load image\n",
    "    image = loader.load_tiff(example_file)\n",
    "    if image is not None:\n",
    "        print(f\"✓ Loaded image shape: {image.shape}\")\n",
    "        \n",
    "        # Segment\n",
    "        masks, seg_info = segmenter.segment_image(image)\n",
    "        if masks is not None:\n",
    "            print(f\"✓ Segmentation complete: {seg_info['num_cells']} cells\")\n",
    "            \n",
    "            # Extract features\n",
    "            features = feature_extractor.extract_all_features(image, masks)\n",
    "            print(f\"✓ Extracted features: {features.shape}\")\n",
    "            print(f\"  Feature columns: {list(features.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segmentation\n",
    "seg_viz = SegmentationVisualizer(output_dir=VIZ_DIR)\n",
    "\n",
    "if 'masks' in locals() and masks is not None:\n",
    "    seg_viz.plot_segmentation_overlay(\n",
    "        image, masks,\n",
    "        title=\"Segmentation Overlay\",\n",
    "        filename=\"example_segmentation.png\"\n",
    "    )\n",
    "    \n",
    "    # Display in notebook\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(os.path.join(VIZ_DIR, \"example_segmentation.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct graph\n",
    "graph_constructor = GraphConstructor(proximity_threshold=50)\n",
    "\n",
    "if 'features' in locals() and not features.empty:\n",
    "    graph = graph_constructor.construct_graph(features, masks)\n",
    "    print(f\"✓ Graph constructed:\")\n",
    "    print(f\"  - Nodes: {graph.number_of_nodes()}\")\n",
    "    print(f\"  - Edges: {graph.number_of_edges()}\")\n",
    "    print(f\"  - Average degree: {2*graph.number_of_edges()/graph.number_of_nodes():.2f}\")\n",
    "    \n",
    "    # Convert to PyTorch Geometric format\n",
    "    pg_converter = PyTorchGeometricConverter()\n",
    "    graph_data = pg_converter.to_pytorch_geometric(graph)\n",
    "    print(f\"\\n✓ PyTorch Geometric format:\")\n",
    "    print(f\"  - Node features shape: {graph_data['x'].shape}\")\n",
    "    print(f\"  - Edge index shape: {graph_data['edge_index'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph\n",
    "graph_viz = GraphVisualizer(output_dir=VIZ_DIR)\n",
    "\n",
    "if 'graph' in locals():\n",
    "    graph_viz.plot_graph(\n",
    "        graph,\n",
    "        title=\"Protein Localization Graph\",\n",
    "        filename=\"example_graph.png\",\n",
    "        layout='spring'\n",
    "    )\n",
    "    \n",
    "    # Display\n",
    "    display(Image(os.path.join(VIZ_DIR, \"example_graph.png\")))\n",
    "    \n",
    "    # Graph statistics\n",
    "    graph_viz.plot_graph_statistics(\n",
    "        graph,\n",
    "        title=\"Graph Statistics\",\n",
    "        filename=\"graph_stats.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Graph-CNN\n",
    "graph_cnn = GraphCNN(\n",
    "    in_channels=20,  # Number of node features\n",
    "    hidden_channels=64,\n",
    "    out_channels=NUM_CLASSES,\n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Graph-CNN initialized\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in graph_cnn.parameters())}\")\n",
    "\n",
    "# VGG-16\n",
    "vgg16 = VGG16Classifier(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained=False,  # Set True if you have internet\n",
    "    in_channels=1\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ VGG-16 initialized\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in vgg16.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training (Demo)\n",
    "\n",
    "Note: This is a demonstration. In practice, you would need a labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset for demonstration\n",
    "print(\"Creating demo dataset...\")\n",
    "\n",
    "# In practice, you would load your actual data here\n",
    "# For demo, create synthetic data\n",
    "dummy_images = [torch.randn(1, 224, 224) for _ in range(100)]\n",
    "dummy_graphs = [graph_data for _ in range(100)]  # Reuse example graph\n",
    "dummy_labels = [torch.randint(0, NUM_CLASSES, (1,)).item() for _ in range(100)]\n",
    "\n",
    "# Create dataset\n",
    "# dataset = ProteinLocalizationDataset(dummy_images, dummy_graphs, dummy_labels)\n",
    "# train_loader, val_loader = create_data_loaders(dataset, train_split=0.8, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"✓ Demo dataset created\")\n",
    "print(\"  Note: Replace with actual labeled data for real training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (commented out for demo)\n",
    "\"\"\"\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(\n",
    "    model=graph_cnn,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    early_stopping_patience=10,\n",
    "    save_dir=MODELS_DIR\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training code ready (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo evaluation with synthetic results\n",
    "metrics_evaluator = MetricsEvaluator(output_dir=VIZ_DIR)\n",
    "\n",
    "# Create synthetic predictions for demo\n",
    "y_true = np.random.randint(0, NUM_CLASSES, 100)\n",
    "y_pred = np.random.randint(0, NUM_CLASSES, 100)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = metrics_evaluator.calculate_all_metrics(\n",
    "    y_true, y_pred,\n",
    "    class_names=[f'Class_{i}' for i in range(NUM_CLASSES)]\n",
    ")\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "print(f\"  Precision: {metrics['precision_avg']:.2f}%\")\n",
    "print(f\"  Recall: {metrics['recall_avg']:.2f}%\")\n",
    "print(f\"  F1-Score: {metrics['f1_avg']:.2f}%\")\n",
    "print(f\"  Specificity: {metrics['specificity_avg']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = np.array(metrics['confusion_matrix'])\n",
    "metrics_evaluator.plot_confusion_matrix(\n",
    "    cm,\n",
    "    class_names=[f'C{i}' for i in range(NUM_CLASSES)],\n",
    "    title=\"Confusion Matrix\",\n",
    "    filename=\"confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "# Display\n",
    "display(Image(os.path.join(VIZ_DIR, \"confusion_matrix.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics comparison\n",
    "metrics_evaluator.plot_metrics_comparison(\n",
    "    metrics,\n",
    "    title=\"Performance Metrics\",\n",
    "    filename=\"metrics_comparison.png\"\n",
    ")\n",
    "\n",
    "display(Image(os.path.join(VIZ_DIR, \"metrics_comparison.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference on New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_localization(image_path, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Complete inference pipeline for a new TIFF image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to TIFF file\n",
    "        model: Trained model\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions and visualizations\n",
    "    \"\"\"\n",
    "    # Load\n",
    "    loader = TIFFLoader()\n",
    "    image = loader.load_tiff(image_path)\n",
    "    \n",
    "    # Segment\n",
    "    segmenter = CellposeSegmenter()\n",
    "    masks, _ = segmenter.segment_image(image)\n",
    "    \n",
    "    # Extract features\n",
    "    extractor = FeatureExtractor()\n",
    "    features = extractor.extract_all_features(image, masks)\n",
    "    \n",
    "    # Build graph\n",
    "    constructor = GraphConstructor()\n",
    "    graph = constructor.construct_graph(features, masks)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    converter = PyTorchGeometricConverter()\n",
    "    graph_data = converter.to_pytorch_geometric(graph)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = graph_data['x'].to(device)\n",
    "        edge_index = graph_data['edge_index'].to(device)\n",
    "        output = model(x, edge_index)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "        confidence = torch.softmax(output, dim=1).max().item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': pred,\n",
    "        'confidence': confidence,\n",
    "        'num_cells': len(features),\n",
    "        'graph_nodes': graph.number_of_nodes(),\n",
    "        'graph_edges': graph.number_of_edges()\n",
    "    }\n",
    "\n",
    "print(\"✓ Inference function ready\")\n",
    "print(\"  Usage: result = predict_localization(image_path, model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PROTEIN SUB-CELLULAR LOCALIZATION PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented Components:\")\n",
    "print(\"  ✓ Preprocessing pipeline with Cellpose segmentation\")\n",
    "print(\"  ✓ Feature extraction (spatial, morphological, intensity)\")\n",
    "print(\"  ✓ Graph construction with PyTorch Geometric\")\n",
    "print(\"  ✓ Graph-CNN and VGG-16 models\")\n",
    "print(\"  ✓ Combined CNN + Graph-CNN architecture\")\n",
    "print(\"  ✓ Training and evaluation framework\")\n",
    "print(\"  ✓ Comprehensive visualization suite\")\n",
    "print(\"  ✓ Web interface with Gradio\")\n",
    "print(\"\\nOutput Directories:\")\n",
    "print(f\"  - Models: {MODELS_DIR}\")\n",
    "print(f\"  - Visualizations: {VIZ_DIR}\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Collect and label training data\")\n",
    "print(\"  2. Train models on labeled dataset\")\n",
    "print(\"  3. Fine-tune hyperparameters\")\n",
    "print(\"  4. Deploy web interface\")\n",
    "print(\"  5. Generate predictions on new data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
