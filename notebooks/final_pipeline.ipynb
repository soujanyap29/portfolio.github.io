{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Sub-Cellular Localization Pipeline\n",
    "## Complete End-to-End Pipeline for 4D TIFF Image Analysis\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Loading and preprocessing 4D TIFF images\n",
    "2. Cellpose segmentation of neuronal structures\n",
    "3. Feature extraction and graph construction\n",
    "4. Graph-CNN model training and evaluation\n",
    "5. Visualization of results\n",
    "6. Final prediction on sample images\n",
    "\n",
    "**Author:** Protein Localization Analysis Team  \n",
    "**Date:** 2024  \n",
    "**Environment:** Ubuntu + JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image processing\n",
    "import tifffile\n",
    "import cv2\n",
    "from skimage import measure, morphology\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from cellpose import models\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from preprocessing.preprocess import PreprocessingPipeline, TIFFProcessor, CellposeSegmenter, FeatureExtractor\n",
    "from graph.graph_builder import GraphBuilder, GraphDataset\n",
    "from models.train import GraphCNN, ModelTrainer, prepare_data_loaders\n",
    "from visualization.plots import VisualizationSuite\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "INPUT_DIR = \"/mnt/d/5TH_SEM/CELLULAR/input\"\n",
    "OUTPUT_DIR = \"/mnt/d/5TH_SEM/CELLULAR/output\"\n",
    "MODELS_DIR = f\"{OUTPUT_DIR}/models\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "NUM_CLASSES = 6  # soma, dendrite, axon, nucleus, synapse, mitochondria\n",
    "CLASS_NAMES = ['Soma', 'Dendrite', 'Axon', 'Nucleus', 'Synapse', 'Mitochondria']\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Input Directory: {INPUT_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Models Directory: {MODELS_DIR}\")\n",
    "print(f\"  Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Training Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "### 3.1 Scan for TIFF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing pipeline\n",
    "preprocessing_pipeline = PreprocessingPipeline(input_dir=INPUT_DIR, output_dir=OUTPUT_DIR)\n",
    "\n",
    "# Scan for TIFF files\n",
    "tiff_files = preprocessing_pipeline.processor.scan_directories()\n",
    "\n",
    "print(f\"\\nFound {len(tiff_files)} TIFF files:\")\n",
    "for i, file in enumerate(tiff_files[:5], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(tiff_files) > 5:\n",
    "    print(f\"  ... and {len(tiff_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Process Sample Image\n",
    "\n",
    "Let's process a single image to understand the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample TIFF file\n",
    "if len(tiff_files) > 0:\n",
    "    sample_file = tiff_files[0]\n",
    "    print(f\"Processing sample file: {sample_file.name}\")\n",
    "    \n",
    "    # Load image\n",
    "    sample_img = preprocessing_pipeline.processor.load_tiff(sample_file)\n",
    "    \n",
    "    print(f\"\\nImage properties:\")\n",
    "    print(f\"  Shape: {sample_img.shape}\")\n",
    "    print(f\"  Dtype: {sample_img.dtype}\")\n",
    "    print(f\"  Min value: {sample_img.min()}\")\n",
    "    print(f\"  Max value: {sample_img.max()}\")\n",
    "    print(f\"  Mean value: {sample_img.mean():.2f}\")\n",
    "else:\n",
    "    print(\"No TIFF files found. Creating synthetic data for demonstration...\")\n",
    "    # Create synthetic data\n",
    "    sample_img = np.random.rand(10, 512, 512) * 255\n",
    "    sample_img = sample_img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Image Segmentation with Cellpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and segment\n",
    "img_norm = preprocessing_pipeline.processor.normalize_image(sample_img)\n",
    "\n",
    "print(\"Running Cellpose segmentation...\")\n",
    "masks, seg_metadata = preprocessing_pipeline.segmenter.segment(img_norm)\n",
    "\n",
    "print(f\"\\nSegmentation results:\")\n",
    "print(f\"  Number of cells detected: {seg_metadata['num_cells']}\")\n",
    "print(f\"  Mask shape: {masks.shape}\")\n",
    "print(f\"  Unique regions: {len(np.unique(masks)) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare 2D image for feature extraction\n",
    "if sample_img.ndim == 4:\n",
    "    img_2d = np.max(sample_img, axis=(0, 1))\n",
    "elif sample_img.ndim == 3:\n",
    "    img_2d = np.max(sample_img, axis=0) if sample_img.shape[-1] > 3 else sample_img\n",
    "else:\n",
    "    img_2d = sample_img\n",
    "\n",
    "# Extract features\n",
    "region_features = preprocessing_pipeline.feature_extractor.extract_region_properties(img_2d, masks)\n",
    "spatial_features = preprocessing_pipeline.feature_extractor.extract_spatial_features(masks)\n",
    "\n",
    "print(f\"\\nExtracted features for {len(region_features)} regions\")\n",
    "print(f\"\\nSample region features:\")\n",
    "if len(region_features) > 0:\n",
    "    sample_features = region_features[0]\n",
    "    for key, value in list(sample_features.items())[:8]:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Process All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images (this may take time)\n",
    "print(\"Processing all TIFF files...\")\n",
    "print(\"Note: This step may take several minutes depending on the number and size of images.\\n\")\n",
    "\n",
    "if len(tiff_files) > 0:\n",
    "    results = preprocessing_pipeline.process_all()\n",
    "    print(f\"\\nSuccessfully preprocessed {len(results)} images\")\n",
    "else:\n",
    "    print(\"Creating demo preprocessed data...\")\n",
    "    # Create demo data\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        demo_result = {\n",
    "            'filename': f'demo_{i}.tif',\n",
    "            'masks': masks,\n",
    "            'region_features': region_features,\n",
    "            'spatial_features': spatial_features,\n",
    "            'num_regions': len(region_features)\n",
    "        }\n",
    "        results.append(demo_result)\n",
    "    \n",
    "    # Save demo data\n",
    "    with open(f\"{OUTPUT_DIR}/preprocessed_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Created {len(results)} demo samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Construction\n",
    "\n",
    "### 4.1 Build Biological Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize graph dataset\n",
    "print(\"Building graphs from preprocessed data...\")\n",
    "graph_dataset = GraphDataset(f\"{OUTPUT_DIR}/preprocessed_data.pkl\")\n",
    "graph_dataset.load_and_build_graphs()\n",
    "\n",
    "# Save graphs\n",
    "graph_dataset.save_graphs(f\"{OUTPUT_DIR}/graphs.pkl\")\n",
    "\n",
    "print(f\"\\nGraph dataset statistics:\")\n",
    "print(f\"  Total graphs: {len(graph_dataset)}\")\n",
    "\n",
    "if len(graph_dataset) > 0:\n",
    "    sample_graph, sample_label = graph_dataset.get_graph(0)\n",
    "    print(f\"\\nSample graph properties:\")\n",
    "    print(f\"  Number of nodes: {sample_graph.num_nodes}\")\n",
    "    print(f\"  Number of edges: {sample_graph.edge_index.shape[1]}\")\n",
    "    print(f\"  Node feature dimension: {sample_graph.x.shape[1]}\")\n",
    "    print(f\"  Label: {sample_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Sample Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample graph\n",
    "if len(graph_dataset) > 0:\n",
    "    # Build NetworkX graph for visualization\n",
    "    sample_result = results[0]\n",
    "    G = graph_dataset.graph_builder.build_graph_from_features(sample_result)\n",
    "    \n",
    "    viz_suite = VisualizationSuite(output_dir=OUTPUT_DIR)\n",
    "    viz_suite.plot_graph_visualization(G, title=\"Sample Biological Graph\")\n",
    "    \n",
    "    print(\"Graph visualization saved!\")\n",
    "    print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {G.number_of_edges()}\")\n",
    "    print(f\"  Density: {nx.density(G):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "### 5.1 Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test loaders\n",
    "print(\"Preparing data loaders...\")\n",
    "\n",
    "# Add dummy labels for demonstration (in real scenario, these would come from annotations)\n",
    "with open(f\"{OUTPUT_DIR}/graphs.pkl\", 'rb') as f:\n",
    "    graph_data = pickle.load(f)\n",
    "\n",
    "graphs = graph_data['graphs']\n",
    "# Create random labels for demonstration\n",
    "labels = np.random.randint(0, NUM_CLASSES, len(graphs))\n",
    "\n",
    "# Add labels to graphs\n",
    "for graph, label in zip(graphs, labels):\n",
    "    graph.y = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "# Update saved data\n",
    "graph_data['labels'] = labels.tolist()\n",
    "with open(f\"{OUTPUT_DIR}/graphs.pkl\", 'wb') as f:\n",
    "    pickle.dump(graph_data, f)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, test_loader = prepare_data_loaders(\n",
    "    f\"{OUTPUT_DIR}/graphs.pkl\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dimension from first batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "num_features = sample_batch.x.shape[1]\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Input features: {num_features}\")\n",
    "print(f\"  Output classes: {NUM_CLASSES}\")\n",
    "print(f\"  Hidden channels: 64\")\n",
    "print(f\"  Number of layers: 3\")\n",
    "\n",
    "# Initialize Graph-CNN model\n",
    "model = GraphCNN(\n",
    "    num_features=num_features,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    hidden_channels=64,\n",
    "    num_layers=3\n",
    ")\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ModelTrainer(model, device=device)\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\\n\")\n",
    "\n",
    "# Train the model\n",
    "metrics = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Save Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_path = f\"{MODELS_DIR}/graph_cnn.pth\"\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = f\"{MODELS_DIR}/metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON\n",
    "    metrics_json = {\n",
    "        k: v.tolist() if isinstance(v, np.ndarray) else v\n",
    "        for k, v in metrics.items()\n",
    "    }\n",
    "    json.dump(metrics_json, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "### 6.1 Display Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Accuracy:    {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision:   {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:      {metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:    {metrics['f1_score']:.4f}\")\n",
    "print(f\"  Specificity: {metrics['specificity']:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Specificity:\")\n",
    "for i, (class_name, spec) in enumerate(zip(CLASS_NAMES, metrics['specificity_per_class'])):\n",
    "    print(f\"  {class_name:15s}: {spec:.4f}\")\n",
    "\n",
    "# Create metrics summary dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity'],\n",
    "    'Value': [\n",
    "        metrics['accuracy'],\n",
    "        metrics['precision'],\n",
    "        metrics['recall'],\n",
    "        metrics['f1_score'],\n",
    "        metrics['specificity']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = np.array(metrics['confusion_matrix'])\n",
    "\n",
    "viz_suite = VisualizationSuite(output_dir=OUTPUT_DIR)\n",
    "viz_suite.plot_confusion_matrix(cm, CLASS_NAMES, save_name=\"confusion_matrix.png\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nConfusion matrix visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "viz_suite.plot_training_history(trainer.history, save_name=\"training_history.png\")\n",
    "\n",
    "print(\"Training history visualization saved!\")\n",
    "print(f\"\\nFinal training accuracy: {trainer.history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {trainer.history['val_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Suite\n",
    "\n",
    "### 7.1 Image Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image overlay visualization\n",
    "if len(results) > 0:\n",
    "    sample_result = results[0]\n",
    "    \n",
    "    # Get original image\n",
    "    if len(tiff_files) > 0:\n",
    "        original_img = tifffile.imread(str(tiff_files[0]))\n",
    "        if original_img.ndim == 4:\n",
    "            original_img = np.max(original_img, axis=(0, 1))\n",
    "        elif original_img.ndim == 3 and original_img.shape[-1] > 3:\n",
    "            original_img = np.max(original_img, axis=0)\n",
    "    else:\n",
    "        original_img = sample_img if sample_img.ndim == 2 else sample_img[0]\n",
    "    \n",
    "    viz_suite.plot_image_overlay(\n",
    "        original_img,\n",
    "        sample_result['masks'],\n",
    "        title=\"Segmentation Overlay\",\n",
    "        save_name=\"sample_overlay.png\"\n",
    "    )\n",
    "    \n",
    "    print(\"Image overlay visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Compartment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic compartment data for visualization\n",
    "compartment_data = {\n",
    "    'Soma': np.random.normal(100, 20, 30).tolist(),\n",
    "    'Dendrite': np.random.normal(80, 15, 35).tolist(),\n",
    "    'Axon': np.random.normal(60, 10, 25).tolist(),\n",
    "    'Nucleus': np.random.normal(120, 25, 20).tolist(),\n",
    "    'Synapse': np.random.normal(40, 8, 40).tolist(),\n",
    "    'Mitochondria': np.random.normal(70, 12, 30).tolist()\n",
    "}\n",
    "\n",
    "# Grouped bar plot\n",
    "viz_suite.plot_grouped_bar_with_points(\n",
    "    compartment_data,\n",
    "    ylabel=\"Mean Intensity (a.u.)\",\n",
    "    title=\"Protein Intensity by Compartment\",\n",
    "    save_name=\"compartment_intensity.png\"\n",
    ")\n",
    "\n",
    "# Box and violin plot\n",
    "viz_suite.plot_box_violin(\n",
    "    compartment_data,\n",
    "    ylabel=\"Intensity Distribution\",\n",
    "    title=\"Compartment Intensity Distribution\",\n",
    "    save_name=\"intensity_distribution.png\"\n",
    ")\n",
    "\n",
    "print(\"Compartment distribution visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Colocalization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dual-channel data\n",
    "channel_a = np.random.rand(512, 512) * 255\n",
    "channel_b = channel_a * 0.7 + np.random.rand(512, 512) * 76.5  # Partially correlated\n",
    "\n",
    "# Colocalization scatter plot\n",
    "viz_suite.plot_colocalization_scatter(\n",
    "    channel_a,\n",
    "    channel_b,\n",
    "    title=\"Channel A vs Channel B Colocalization\",\n",
    "    save_name=\"colocalization.png\"\n",
    ")\n",
    "\n",
    "# Colocalization metrics\n",
    "coloc_metrics = {\n",
    "    'Pearson': np.corrcoef(channel_a.flatten(), channel_b.flatten())[0, 1],\n",
    "    'Manders M1': 0.72,\n",
    "    'Manders M2': 0.68,\n",
    "    'Overlap': 0.75\n",
    "}\n",
    "\n",
    "viz_suite.plot_colocalization_metrics(\n",
    "    coloc_metrics,\n",
    "    save_name=\"coloc_metrics.png\"\n",
    ")\n",
    "\n",
    "print(\"Colocalization visualizations saved!\")\n",
    "print(f\"\\nColocalization metrics:\")\n",
    "for key, value in coloc_metrics.items():\n",
    "    print(f\"  {key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Intensity Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic intensity profile data\n",
    "distances = np.linspace(0, 200, 100)\n",
    "intensities = 100 * np.exp(-distances / 50) + np.random.normal(0, 5, 100)\n",
    "\n",
    "viz_suite.plot_intensity_profile(\n",
    "    distances,\n",
    "    intensities,\n",
    "    title=\"Protein Intensity vs Distance from Soma\",\n",
    "    save_name=\"intensity_profile.png\"\n",
    ")\n",
    "\n",
    "print(\"Intensity profile visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Prediction Demo\n",
    "\n",
    "### 8.1 Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "inference_model = GraphCNN(num_features=num_features, num_classes=NUM_CLASSES)\n",
    "inference_trainer = ModelTrainer(inference_model, device=device)\n",
    "inference_trainer.load_model(f\"{MODELS_DIR}/graph_cnn.pth\")\n",
    "\n",
    "inference_model.eval()\n",
    "print(\"Model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Predict on Sample TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test sample\n",
    "test_sample = next(iter(test_loader))\n",
    "test_sample = test_sample.to(device)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    output = inference_model(test_sample.x, test_sample.edge_index, test_sample.batch)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predictions = output.argmax(dim=1)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "    predicted_class = CLASS_NAMES[pred.item()]\n",
    "    confidence = prob[pred].item()\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Predicted Class: {predicted_class}\")\n",
    "    print(f\"  Confidence: {confidence:.2%}\")\n",
    "    print(f\"\\n  Class Probabilities:\")\n",
    "    for class_name, p in zip(CLASS_NAMES, prob):\n",
    "        print(f\"    {class_name:15s}: {p.item():.2%}\")\n",
    "    \n",
    "    if i >= 2:  # Show first 3 samples\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### 9.1 Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "âœ… Data Processing:\n",
    "   - Processed {len(results)} TIFF images\n",
    "   - Segmented {sum(r['num_regions'] for r in results)} total regions\n",
    "   - Extracted morphological and intensity features\n",
    "\n",
    "âœ… Graph Construction:\n",
    "   - Built {len(graph_dataset)} biological graphs\n",
    "   - Average nodes per graph: {np.mean([g.num_nodes for g in graphs]):.1f}\n",
    "   - Average edges per graph: {np.mean([g.edge_index.shape[1] for g in graphs]):.1f}\n",
    "\n",
    "âœ… Model Training:\n",
    "   - Trained Graph-CNN with {total_params:,} parameters\n",
    "   - Final accuracy: {metrics['accuracy']:.2%}\n",
    "   - F1-Score: {metrics['f1_score']:.4f}\n",
    "\n",
    "âœ… Outputs Saved:\n",
    "   - Preprocessed data: {OUTPUT_DIR}/preprocessed_data.pkl\n",
    "   - Graph data: {OUTPUT_DIR}/graphs.pkl\n",
    "   - Trained model: {MODELS_DIR}/graph_cnn.pth\n",
    "   - Metrics: {MODELS_DIR}/metrics.json\n",
    "   - Visualizations: {OUTPUT_DIR}/*.png\n",
    "\n",
    "ðŸ“Š Performance Metrics:\n",
    "   - Accuracy:    {metrics['accuracy']:.4f}\n",
    "   - Precision:   {metrics['precision']:.4f}\n",
    "   - Recall:      {metrics['recall']:.4f}\n",
    "   - F1-Score:    {metrics['f1_score']:.4f}\n",
    "   - Specificity: {metrics['specificity']:.4f}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Pipeline execution completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Next Steps and Recommendations\n",
    "\n",
    "1. **Improve Model Performance:**\n",
    "   - Collect more annotated training data\n",
    "   - Try Graph Attention Networks (GAT)\n",
    "   - Implement data augmentation\n",
    "   - Fine-tune hyperparameters\n",
    "\n",
    "2. **Advanced Analysis:**\n",
    "   - Implement temporal analysis for 4D data\n",
    "   - Add multi-channel colocalization\n",
    "   - Develop hierarchical compartment models\n",
    "\n",
    "3. **Deployment:**\n",
    "   - Use the Flask web interface: `python src/frontend/app.py`\n",
    "   - Deploy on cloud platform\n",
    "   - Create batch processing pipeline\n",
    "\n",
    "4. **Documentation:**\n",
    "   - Add more inline comments\n",
    "   - Create user guide\n",
    "   - Document model architecture choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. References and Citations\n",
    "\n",
    "- **Cellpose:** Stringer, C., et al. (2021). \"Cellpose: a generalist algorithm for cellular segmentation.\" Nature Methods.\n",
    "- **PyTorch Geometric:** Fey, M., & Lenssen, J. E. (2019). \"Fast Graph Representation Learning with PyTorch Geometric.\"\n",
    "- **Graph Neural Networks:** Kipf, T. N., & Welling, M. (2017). \"Semi-Supervised Classification with Graph Convolutional Networks.\"\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
